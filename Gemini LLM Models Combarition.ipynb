{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49707c81",
   "metadata": {},
   "source": [
    "#### Cell 1 — Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052b224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (1.33.0)\n",
      "Collecting google-genai\n",
      "  Downloading google_genai-1.35.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: tabulate in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (4.8.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-genai) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\nirosh\\anaconda3\\envs\\cits5508\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.35.0-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: google-genai\n",
      "  Attempting uninstall: google-genai\n",
      "    Found existing installation: google-genai 1.33.0\n",
      "    Uninstalling google-genai-1.33.0:\n",
      "      Successfully uninstalled google-genai-1.33.0\n",
      "Successfully installed google-genai-1.35.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U google-genai python-dotenv pandas tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdde0a3",
   "metadata": {},
   "source": [
    "#### Cell 2 — Load API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ca9854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY: AIza...2-E8\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Change \"keys.env\" to \".env\" if that's your filename\n",
    "load_dotenv(\"keys.env\")\n",
    "\n",
    "def masked(v): \n",
    "    return v[:4] + \"...\" + v[-4:] if v and len(v) >= 8 else str(bool(v))\n",
    "\n",
    "print(\"GEMINI_API_KEY:\", masked(os.getenv(\"GEMINI_API_KEY\")))\n",
    "assert os.getenv(\"GEMINI_API_KEY\"), \"Missing GEMINI_API_KEY in keys.env/.env\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab674a",
   "metadata": {},
   "source": [
    "#### Cell 3 — Imports & global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7512c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math, statistics as stats, pandas as pd\n",
    "from tabulate import tabulate\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Global defaults (you can tweak in experiments)\n",
    "PROMPT = \"Explain transformers in AI in 3 short sentences.\"\n",
    "N_RUNS = 5                 # repeats per model (excluding warm-up)\n",
    "MAX_OUTPUT_TOKENS = 128    # default answer length\n",
    "TEMPERATURE = 0.5          # moderately deterministic\n",
    "\n",
    "client = genai.Client()    # uses GEMINI_API_KEY from env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c412b",
   "metadata": {},
   "source": [
    "#### Cell 4 — Model helpers (filters & rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b22933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def needs_thinking(model_id: str) -> bool:\n",
    "    \"\"\"Gemini 2.5 Pro family requires thinking mode (budget 128..32768).\"\"\"\n",
    "    return \"gemini-2.5-pro\" in model_id\n",
    "\n",
    "def disallow_thinking(model_id: str) -> bool:\n",
    "    \"\"\"Gemma family does not support thinking_config.\"\"\"\n",
    "    return model_id.startswith(\"models/gemma-\")\n",
    "\n",
    "def is_text_model(model_id: str) -> bool:\n",
    "    \"\"\"Filter out non-text models (imagen/veo/embeddings/tts/live/image).\"\"\"\n",
    "    bad = [\"imagen\", \"veo\", \"embedding\", \"aqa\", \"tts\", \"live\", \"image\"]\n",
    "    return all(b not in model_id for b in bad)\n",
    "\n",
    "def normalize_ids(ids):\n",
    "    def norm(mid):\n",
    "        return mid if mid.startswith(\"models/\") else f\"models/{mid}\"\n",
    "    return [norm(m) for m in ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ce29e",
   "metadata": {},
   "source": [
    "#### Cell 5 — Core runner (streaming + conditional thinking + retries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2765b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(model_id: str, prompt: str,\n",
    "             max_output_tokens: int = None,\n",
    "             temperature: float = None,\n",
    "             candidate_count: int = 1,\n",
    "             pro_thinking_budget: int = 256) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Streams the response and measures:\n",
    "      - ttfb_s (time to first token)\n",
    "      - time_to_final_token_s (first → last token)\n",
    "      - total_s (end-to-end)\n",
    "    Applies:\n",
    "      - thinking_config=pro_thinking_budget for 2.5 Pro (128..32768)\n",
    "      - thinking_config omitted for Gemma\n",
    "      - thinking_config=0 for other Gemini\n",
    "    Retries transient 429/503 errors.\n",
    "    \"\"\"\n",
    "    retries = 2\n",
    "    backoff = 1.5\n",
    "    attempt = 0\n",
    "\n",
    "    max_output_tokens = MAX_OUTPUT_TOKENS if max_output_tokens is None else max_output_tokens\n",
    "    temperature = TEMPERATURE if temperature is None else temperature\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            first = None\n",
    "            last = None\n",
    "            chunks = []\n",
    "\n",
    "            cfg = dict(\n",
    "                temperature=temperature,\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                candidate_count=candidate_count,\n",
    "            )\n",
    "            if needs_thinking(model_id):\n",
    "                cfg[\"thinking_config\"] = types.ThinkingConfig(thinking_budget=pro_thinking_budget)\n",
    "            elif disallow_thinking(model_id):\n",
    "                pass  # omit thinking_config\n",
    "            else:\n",
    "                cfg[\"thinking_config\"] = types.ThinkingConfig(thinking_budget=0)\n",
    "\n",
    "            stream = client.models.generate_content_stream(\n",
    "                model=model_id,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(**cfg),\n",
    "            )\n",
    "\n",
    "            for chunk in stream:\n",
    "                if chunk.text:\n",
    "                    now = time.perf_counter()\n",
    "                    if first is None:\n",
    "                        first = now\n",
    "                    last = now\n",
    "                    chunks.append(chunk.text)\n",
    "\n",
    "            total_s = time.perf_counter() - t0\n",
    "            ttfb_s = (first - t0) if first else math.nan\n",
    "            time_to_final_token_s = (last - first) if (first and last) else math.nan\n",
    "\n",
    "            return {\n",
    "                \"model\": model_id,\n",
    "                \"ttfb_s\": ttfb_s,\n",
    "                \"time_to_final_token_s\": time_to_final_token_s,\n",
    "                \"total_s\": total_s,\n",
    "                \"text\": \"\".join(chunks),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            transient = (\"RESOURCE_EXHAUSTED\" in msg) or (\"UNAVAILABLE\" in msg) or (\"429\" in msg) or (\"503\" in msg)\n",
    "            if transient and attempt < retries:\n",
    "                attempt += 1\n",
    "                time.sleep(backoff * attempt)\n",
    "                continue\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc47510",
   "metadata": {},
   "source": [
    "#### Cell 6 — Benchmark function (table + CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64350e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bench_models(models: List[str], prompt: str,\n",
    "                 max_output_tokens: int = None,\n",
    "                 temperature: float = None,\n",
    "                 candidate_count: int = 1,\n",
    "                 pro_thinking_budget: int = 256,\n",
    "                 runs: int = None,\n",
    "                 csv_path: str = None,\n",
    "                 warmup_runs: int = 2) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    runs = N_RUNS if runs is None else runs\n",
    "\n",
    "    for mid in models:\n",
    "        print(f\"\\n--- {mid} ---\")\n",
    "        status = \"ok\"\n",
    "        try:\n",
    "            # Warm-ups to reduce cold-start variance\n",
    "            for _ in range(max(0, warmup_runs)):\n",
    "                _ = run_once(mid, prompt,\n",
    "                             max_output_tokens=max_output_tokens,\n",
    "                             temperature=temperature,\n",
    "                             candidate_count=candidate_count,\n",
    "                             pro_thinking_budget=pro_thinking_budget)\n",
    "\n",
    "            # Measured runs\n",
    "            times = [run_once(mid, prompt,\n",
    "                              max_output_tokens=max_output_tokens,\n",
    "                              temperature=temperature,\n",
    "                              candidate_count=candidate_count,\n",
    "                              pro_thinking_budget=pro_thinking_budget)\n",
    "                     for _ in range(runs)]\n",
    "\n",
    "            ttfb_vals = [t[\"ttfb_s\"] for t in times if not math.isnan(t[\"ttfb_s\"])]\n",
    "            t2last_vals = [t[\"time_to_final_token_s\"] for t in times if not math.isnan(t[\"time_to_final_token_s\"])]\n",
    "            total_vals = [t[\"total_s\"] for t in times]\n",
    "\n",
    "            def summary(xs: List[float]) -> Tuple[float,float,float]:\n",
    "                return (min(xs), sum(xs)/len(xs), max(xs))\n",
    "\n",
    "            ttfb = summary(ttfb_vals) if ttfb_vals else (math.nan, math.nan, math.nan)\n",
    "            t2last = summary(t2last_vals) if t2last_vals else (math.nan, math.nan, math.nan)\n",
    "            total = summary(total_vals)\n",
    "\n",
    "            print(f\"TTFB          min/avg/max: {ttfb}\")\n",
    "            print(f\"Time-to-final min/avg/max: {t2last}\")\n",
    "            print(f\"Total         min/avg/max: {total}\")\n",
    "\n",
    "            rows.append({\n",
    "                \"model\": mid,\n",
    "                \"status\": status,\n",
    "                \"ttfb_min\": ttfb[0], \"ttfb_avg\": ttfb[1], \"ttfb_max\": ttfb[2],\n",
    "                \"t2last_min\": t2last[0], \"t2last_avg\": t2last[1], \"t2last_max\": t2last[2],\n",
    "                \"total_min\": total[0], \"total_avg\": total[1], \"total_max\": total[2],\n",
    "                \"max_output_tokens\": max_output_tokens if max_output_tokens is not None else MAX_OUTPUT_TOKENS,\n",
    "                \"temperature\": temperature if temperature is not None else TEMPERATURE,\n",
    "                \"candidate_count\": candidate_count,\n",
    "                \"pro_thinking_budget\": pro_thinking_budget if needs_thinking(mid) else 0,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            status = str(e).split(\"\\n\", 1)[0][:200]  # short error\n",
    "            print(f\"Error: {status}\")\n",
    "            rows.append({\n",
    "                \"model\": mid,\n",
    "                \"status\": status,\n",
    "                \"ttfb_min\": None, \"ttfb_avg\": None, \"ttfb_max\": None,\n",
    "                \"t2last_min\": None, \"t2last_avg\": None, \"t2last_max\": None,\n",
    "                \"total_min\": None, \"total_avg\": None, \"total_max\": None,\n",
    "                \"max_output_tokens\": max_output_tokens if max_output_tokens is not None else MAX_OUTPUT_TOKENS,\n",
    "                \"temperature\": temperature if temperature is not None else TEMPERATURE,\n",
    "                \"candidate_count\": candidate_count,\n",
    "                \"pro_thinking_budget\": pro_thinking_budget if needs_thinking(mid) else 0,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"model\",\"status\",\n",
    "            \"ttfb_min\",\"ttfb_avg\",\"ttfb_max\",\n",
    "            \"t2last_min\",\"t2last_avg\",\"t2last_max\",\n",
    "            \"total_min\",\"total_avg\",\"total_max\",\n",
    "            \"max_output_tokens\",\"temperature\",\"candidate_count\",\"pro_thinking_budget\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Save the full (including errors) table\n",
    "    if csv_path:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nSaved full results (including errors): {csv_path}\")\n",
    "\n",
    "    # Show a clean table with only successful rows\n",
    "    ok_df = df[df[\"status\"] == \"ok\"].copy()\n",
    "    if not ok_df.empty:\n",
    "        ok_df = ok_df.sort_values(\"total_avg\", na_position=\"last\")\n",
    "        print(\"\\n=== Successful models (sorted by total_avg) ===\")\n",
    "        from tabulate import tabulate\n",
    "        print(tabulate(ok_df, headers=\"keys\", tablefmt=\"github\", floatfmt=\".3f\"))\n",
    "    else:\n",
    "        print(\"\\nNo successful rows to show.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2418323",
   "metadata": {},
   "source": [
    "#### Cell 7 — Choose models to test (from your list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7e00e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing these models: ['models/gemini-1.5-flash', 'models/gemini-1.5-pro', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-lite', 'models/gemini-2.5-flash', 'models/gemini-2.5-flash-lite', 'models/gemini-2.5-pro', 'models/gemma-3-4b-it']\n"
     ]
    }
   ],
   "source": [
    "# Representative set (edit as needed)\n",
    "wanted = normalize_ids([\n",
    "    # Gemini 1.5 (older; often free-tier)\n",
    "    \"gemini-1.5-flash\",\n",
    "    \"gemini-1.5-pro\",      # may be throttled on free tier\n",
    "\n",
    "    # Gemini 2.0 (legacy/experimental speed tiers)\n",
    "    \"gemini-2.0-flash\",\n",
    "    \"gemini-2.0-flash-lite\",\n",
    "\n",
    "    # Gemini 2.5 (current mainline; best for production)\n",
    "    \"gemini-2.5-flash\",\n",
    "    \"gemini-2.5-flash-lite\",\n",
    "    \"gemini-2.5-pro\",      # requires thinking mode (handled automatically)\n",
    "\n",
    "    # Optional: open-weight via API (omit if not needed)\n",
    "    \"gemma-3-4b-it\",\n",
    "])\n",
    "\n",
    "available = {m.name for m in client.models.list()}\n",
    "to_test = [m for m in wanted if (m in available and is_text_model(m))]\n",
    "\n",
    "print(\"Testing these models:\", to_test)\n",
    "assert len(to_test) > 0, \"After filtering, no models remain. Check model IDs or permissions.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f56971",
   "metadata": {},
   "source": [
    "#### Cell 8 — Baseline benchmark + CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64eba02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-1.5-flash ---\n",
      "TTFB          min/avg/max: (2.4288187000202015, 3.545865833371257, 4.148669500020333)\n",
      "Time-to-final min/avg/max: (0.42520169995259494, 0.45863549997253966, 0.48275339999236166)\n",
      "Total         min/avg/max: (2.901229300070554, 4.007619966675217, 4.631613799952902)\n",
      "\n",
      "--- models/gemini-1.5-pro ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "--- models/gemini-2.0-flash ---\n",
      "TTFB          min/avg/max: (0.7008652999065816, 0.7529738333153849, 0.8268949999473989)\n",
      "Time-to-final min/avg/max: (0.3411140999523923, 0.43472943334685016, 0.6056403000839055)\n",
      "Total         min/avg/max: (1.0630029999883845, 1.190409666664588, 1.432850499986671)\n",
      "\n",
      "--- models/gemini-2.0-flash-lite ---\n",
      "TTFB          min/avg/max: (0.4786669999593869, 0.5195707000093535, 0.5643330999882892)\n",
      "Time-to-final min/avg/max: (0.4255022000288591, 0.5578213666643327, 0.6539961999515072)\n",
      "Total         min/avg/max: (0.9900053000310436, 1.0800384333512436, 1.1356388999847695)\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "TTFB          min/avg/max: (0.5560266999527812, 0.6030584666489934, 0.6931202999548987)\n",
      "Time-to-final min/avg/max: (0.41769290005322546, 0.4536352000432089, 0.484321300056763)\n",
      "Total         min/avg/max: (0.9782034999225289, 1.0626286333426833, 1.1552124000154436)\n",
      "\n",
      "--- models/gemini-2.5-flash-lite ---\n",
      "TTFB          min/avg/max: (0.5673948000185192, 0.6860063999968892, 0.8313925999682397)\n",
      "Time-to-final min/avg/max: (0.16548800002783537, 0.1872768666750441, 0.22150340001098812)\n",
      "Total         min/avg/max: (0.7904411000199616, 0.8759360000258312, 0.9971302000340074)\n",
      "\n",
      "--- models/gemini-2.5-pro ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "--- models/gemma-3-4b-it ---\n",
      "TTFB          min/avg/max: (0.9774338999995962, 1.1038771333405748, 1.2396041000029072)\n",
      "Time-to-final min/avg/max: (1.5997699999716133, 1.6606035000489403, 1.723821800085716)\n",
      "Total         min/avg/max: (2.7191432000836357, 2.915121900034137, 3.1014340999536216)\n",
      "\n",
      "Saved full results (including errors): gemini_latency_baseline.csv\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                        | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|------------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  5 | models/gemini-2.5-flash-lite | ok       |      0.567 |      0.686 |      0.831 |        0.165 |        0.187 |        0.222 |       0.790 |       0.876 |       0.997 |                 128 |         0.500 |                 1 |                     0 |\n",
      "|  4 | models/gemini-2.5-flash      | ok       |      0.556 |      0.603 |      0.693 |        0.418 |        0.454 |        0.484 |       0.978 |       1.063 |       1.155 |                 128 |         0.500 |                 1 |                     0 |\n",
      "|  3 | models/gemini-2.0-flash-lite | ok       |      0.479 |      0.520 |      0.564 |        0.426 |        0.558 |        0.654 |       0.990 |       1.080 |       1.136 |                 128 |         0.500 |                 1 |                     0 |\n",
      "|  2 | models/gemini-2.0-flash      | ok       |      0.701 |      0.753 |      0.827 |        0.341 |        0.435 |        0.606 |       1.063 |       1.190 |       1.433 |                 128 |         0.500 |                 1 |                     0 |\n",
      "|  7 | models/gemma-3-4b-it         | ok       |      0.977 |      1.104 |      1.240 |        1.600 |        1.661 |        1.724 |       2.719 |       2.915 |       3.101 |                 128 |         0.500 |                 1 |                     0 |\n",
      "|  0 | models/gemini-1.5-flash      | ok       |      2.429 |      3.546 |      4.149 |        0.425 |        0.459 |        0.483 |       2.901 |       4.008 |       4.632 |                 128 |         0.500 |                 1 |                     0 |\n"
     ]
    }
   ],
   "source": [
    "df_base = bench_models(\n",
    "    to_test,\n",
    "    PROMPT,\n",
    "    max_output_tokens=128,\n",
    "    temperature=0.5,\n",
    "    candidate_count=1,\n",
    "    pro_thinking_budget=256,\n",
    "    runs=3,\n",
    "    csv_path=\"gemini_latency_baseline.csv\",  # full table (incl. errors)\n",
    "    warmup_runs=2,                            # NEW: stabilise streaming\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd3208",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4edc3",
   "metadata": {},
   "source": [
    "#### A) Output length — max_output_tokens sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0236eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "TTFB          min/avg/max: (0.5129429999506101, 0.5990121666885292, 0.6883639000589028)\n",
      "Time-to-final min/avg/max: (0.15281190001405776, 0.22903590001321086, 0.2991009000688791)\n",
      "Total         min/avg/max: (0.7512671999866143, 0.8302920333032185, 0.9254178999690339)\n",
      "\n",
      "--- models/gemini-2.5-flash-lite ---\n",
      "TTFB          min/avg/max: (0.5537831999827176, 0.5921766666773086, 0.622090799966827)\n",
      "Time-to-final min/avg/max: (0.14221620000898838, 0.3474687999890496, 0.7561650000279769)\n",
      "Total         min/avg/max: (0.7461225000442937, 0.9412760333313296, 1.3101708999602124)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                        | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|------------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  0 | models/gemini-2.5-flash      | ok       |      0.513 |      0.599 |      0.688 |        0.153 |        0.229 |        0.299 |       0.751 |       0.830 |       0.925 |                  32 |         0.500 |                 1 |                     0 |\n",
      "|  1 | models/gemini-2.5-flash-lite | ok       |      0.554 |      0.592 |      0.622 |        0.142 |        0.347 |        0.756 |       0.746 |       0.941 |       1.310 |                  32 |         0.500 |                 1 |                     0 |\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "TTFB          min/avg/max: (0.4968317999737337, 0.5822601333493367, 0.644070400041528)\n",
      "Time-to-final min/avg/max: (0.37597870000172406, 0.40218379996561754, 0.43346139998175204)\n",
      "Total         min/avg/max: (0.8729726999299601, 0.9849606666636342, 1.0787115000421181)\n",
      "\n",
      "--- models/gemini-2.5-flash-lite ---\n",
      "TTFB          min/avg/max: (0.46991879993584007, 0.5525916999516388, 0.5997820999473333)\n",
      "Time-to-final min/avg/max: (0.2109045999823138, 0.2360335000169774, 0.255745500093326)\n",
      "Total         min/avg/max: (0.725958599941805, 0.7906698332711434, 0.8321517999283969)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                        | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|------------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  1 | models/gemini-2.5-flash-lite | ok       |      0.470 |      0.553 |      0.600 |        0.211 |        0.236 |        0.256 |       0.726 |       0.791 |       0.832 |                  64 |         0.500 |                 1 |                     0 |\n",
      "|  0 | models/gemini-2.5-flash      | ok       |      0.497 |      0.582 |      0.644 |        0.376 |        0.402 |        0.433 |       0.873 |       0.985 |       1.079 |                  64 |         0.500 |                 1 |                     0 |\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "TTFB          min/avg/max: (0.5371313000796363, 0.5639062333696833, 0.5972809999948367)\n",
      "Time-to-final min/avg/max: (0.39462419995106757, 0.4291780332957084, 0.4519120999611914)\n",
      "Total         min/avg/max: (0.9807350999908522, 0.9948352999829998, 1.0093816999578848)\n",
      "\n",
      "--- models/gemini-2.5-flash-lite ---\n",
      "TTFB          min/avg/max: (0.5378249000059441, 0.5902555999734128, 0.6576422998914495)\n",
      "Time-to-final min/avg/max: (0.1864553999621421, 0.2285738333206003, 0.28100209997501224)\n",
      "Total         min/avg/max: (0.7244572999188676, 0.8198204999401545, 0.8761391999432817)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                        | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|------------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  1 | models/gemini-2.5-flash-lite | ok       |      0.538 |      0.590 |      0.658 |        0.186 |        0.229 |        0.281 |       0.724 |       0.820 |       0.876 |                 128 |         0.500 |                 1 |                     0 |\n",
      "|  0 | models/gemini-2.5-flash      | ok       |      0.537 |      0.564 |      0.597 |        0.395 |        0.429 |        0.452 |       0.981 |       0.995 |       1.009 |                 128 |         0.500 |                 1 |                     0 |\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "--- models/gemini-2.5-flash-lite ---\n",
      "TTFB          min/avg/max: (0.5188460000790656, 0.5794255333409334, 0.6157494999933988)\n",
      "Time-to-final min/avg/max: (0.19927670003380626, 0.21587760001420975, 0.24590839992742985)\n",
      "Total         min/avg/max: (0.7680767000420019, 0.7969562667033946, 0.8152880000416189)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                        | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|------------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  1 | models/gemini-2.5-flash-lite | ok       |      0.519 |      0.579 |      0.616 |        0.199 |        0.216 |        0.246 |       0.768 |       0.797 |       0.815 |                 256 |         0.500 |                 1 |                     0 |\n",
      "\n",
      "Saved: gemini_sweep_tokens.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>ttfb_min</th>\n",
       "      <th>ttfb_avg</th>\n",
       "      <th>ttfb_max</th>\n",
       "      <th>t2last_min</th>\n",
       "      <th>t2last_avg</th>\n",
       "      <th>t2last_max</th>\n",
       "      <th>total_min</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_max</th>\n",
       "      <th>max_output_tokens</th>\n",
       "      <th>temperature</th>\n",
       "      <th>candidate_count</th>\n",
       "      <th>pro_thinking_budget</th>\n",
       "      <th>sweep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.512943</td>\n",
       "      <td>0.599012</td>\n",
       "      <td>0.688364</td>\n",
       "      <td>0.152812</td>\n",
       "      <td>0.229036</td>\n",
       "      <td>0.299101</td>\n",
       "      <td>0.751267</td>\n",
       "      <td>0.830292</td>\n",
       "      <td>0.925418</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>max_tokens=32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-flash-lite</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.553783</td>\n",
       "      <td>0.592177</td>\n",
       "      <td>0.622091</td>\n",
       "      <td>0.142216</td>\n",
       "      <td>0.347469</td>\n",
       "      <td>0.756165</td>\n",
       "      <td>0.746123</td>\n",
       "      <td>0.941276</td>\n",
       "      <td>1.310171</td>\n",
       "      <td>32</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>max_tokens=32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.496832</td>\n",
       "      <td>0.582260</td>\n",
       "      <td>0.644070</td>\n",
       "      <td>0.375979</td>\n",
       "      <td>0.402184</td>\n",
       "      <td>0.433461</td>\n",
       "      <td>0.872973</td>\n",
       "      <td>0.984961</td>\n",
       "      <td>1.078712</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>max_tokens=64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models/gemini-2.5-flash-lite</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.469919</td>\n",
       "      <td>0.552592</td>\n",
       "      <td>0.599782</td>\n",
       "      <td>0.210905</td>\n",
       "      <td>0.236034</td>\n",
       "      <td>0.255746</td>\n",
       "      <td>0.725959</td>\n",
       "      <td>0.790670</td>\n",
       "      <td>0.832152</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>max_tokens=64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.537131</td>\n",
       "      <td>0.563906</td>\n",
       "      <td>0.597281</td>\n",
       "      <td>0.394624</td>\n",
       "      <td>0.429178</td>\n",
       "      <td>0.451912</td>\n",
       "      <td>0.980735</td>\n",
       "      <td>0.994835</td>\n",
       "      <td>1.009382</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>max_tokens=128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model status  ttfb_min  ttfb_avg  ttfb_max  \\\n",
       "0       models/gemini-2.5-flash     ok  0.512943  0.599012  0.688364   \n",
       "1  models/gemini-2.5-flash-lite     ok  0.553783  0.592177  0.622091   \n",
       "2       models/gemini-2.5-flash     ok  0.496832  0.582260  0.644070   \n",
       "3  models/gemini-2.5-flash-lite     ok  0.469919  0.552592  0.599782   \n",
       "4       models/gemini-2.5-flash     ok  0.537131  0.563906  0.597281   \n",
       "\n",
       "   t2last_min  t2last_avg  t2last_max  total_min  total_avg  total_max  \\\n",
       "0    0.152812    0.229036    0.299101   0.751267   0.830292   0.925418   \n",
       "1    0.142216    0.347469    0.756165   0.746123   0.941276   1.310171   \n",
       "2    0.375979    0.402184    0.433461   0.872973   0.984961   1.078712   \n",
       "3    0.210905    0.236034    0.255746   0.725959   0.790670   0.832152   \n",
       "4    0.394624    0.429178    0.451912   0.980735   0.994835   1.009382   \n",
       "\n",
       "   max_output_tokens  temperature  candidate_count  pro_thinking_budget  \\\n",
       "0                 32          0.5                1                    0   \n",
       "1                 32          0.5                1                    0   \n",
       "2                 64          0.5                1                    0   \n",
       "3                 64          0.5                1                    0   \n",
       "4                128          0.5                1                    0   \n",
       "\n",
       "            sweep  \n",
       "0   max_tokens=32  \n",
       "1   max_tokens=32  \n",
       "2   max_tokens=64  \n",
       "3   max_tokens=64  \n",
       "4  max_tokens=128  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_MODELS_A = [\"models/gemini-2.5-flash\", \"models/gemini-2.5-flash-lite\"]\n",
    "TOKENS = [32, 64, 128, 256]\n",
    "\n",
    "frames = []\n",
    "for tok in TOKENS:\n",
    "    df = bench_models(TEST_MODELS_A, PROMPT, max_output_tokens=tok, temperature=0.5,\n",
    "                      candidate_count=1, pro_thinking_budget=256, runs=3)\n",
    "    df[\"sweep\"] = f\"max_tokens={tok}\"\n",
    "    frames.append(df)\n",
    "\n",
    "df_tokens = pd.concat(frames, ignore_index=True)\n",
    "df_tokens.to_csv(\"gemini_sweep_tokens.csv\", index=False)\n",
    "print(\"\\nSaved: gemini_sweep_tokens.csv\")\n",
    "df_tokens.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415db536",
   "metadata": {},
   "source": [
    "#### B) Prompt size — small vs large input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81362f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "Saved: gemini_sweep_prompt_size.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>ttfb_min</th>\n",
       "      <th>ttfb_avg</th>\n",
       "      <th>ttfb_max</th>\n",
       "      <th>t2last_min</th>\n",
       "      <th>t2last_avg</th>\n",
       "      <th>t2last_max</th>\n",
       "      <th>total_min</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_max</th>\n",
       "      <th>max_output_tokens</th>\n",
       "      <th>temperature</th>\n",
       "      <th>candidate_count</th>\n",
       "      <th>pro_thinking_budget</th>\n",
       "      <th>sweep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>repeats=0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>repeats=20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>repeats=50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>repeats=100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                                             status  \\\n",
       "0  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "1  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "2  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "3  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "\n",
       "  ttfb_min ttfb_avg ttfb_max t2last_min t2last_avg t2last_max total_min  \\\n",
       "0     None     None     None       None       None       None      None   \n",
       "1     None     None     None       None       None       None      None   \n",
       "2     None     None     None       None       None       None      None   \n",
       "3     None     None     None       None       None       None      None   \n",
       "\n",
       "  total_avg total_max  max_output_tokens  temperature  candidate_count  \\\n",
       "0      None      None                128          0.5                1   \n",
       "1      None      None                128          0.5                1   \n",
       "2      None      None                128          0.5                1   \n",
       "3      None      None                128          0.5                1   \n",
       "\n",
       "   pro_thinking_budget        sweep  \n",
       "0                    0    repeats=0  \n",
       "1                    0   repeats=20  \n",
       "2                    0   repeats=50  \n",
       "3                    0  repeats=100  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PROMPT = \"Explain transformers in AI in 3 short sentences.\"\n",
    "FILLER = \" The quick brown fox jumps over the lazy dog.\"  # ~45 chars\n",
    "\n",
    "def make_prompt(repeats):\n",
    "    return BASE_PROMPT + FILLER * repeats\n",
    "\n",
    "REPEATS = [0, 20, 50, 100]\n",
    "MODEL_B = \"models/gemini-2.5-flash\"\n",
    "\n",
    "frames = []\n",
    "for r in REPEATS:\n",
    "    p = make_prompt(r)\n",
    "    df = bench_models([MODEL_B], p, max_output_tokens=128, temperature=0.5,\n",
    "                      candidate_count=1, pro_thinking_budget=256, runs=3)\n",
    "    df[\"sweep\"] = f\"repeats={r}\"\n",
    "    frames.append(df)\n",
    "\n",
    "df_prompt = pd.concat(frames, ignore_index=True)\n",
    "df_prompt.to_csv(\"gemini_sweep_prompt_size.csv\", index=False)\n",
    "print(\"\\nSaved: gemini_sweep_prompt_size.csv\")\n",
    "df_prompt.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe50f3a",
   "metadata": {},
   "source": [
    "#### C) Thinking budget (2.5 Pro only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dba42c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-2.5-pro ---\n",
      "TTFB          min/avg/max: (3.3087566000176594, 7.001742099993862, 11.6514549999265)\n",
      "Time-to-final min/avg/max: (0.20669559994712472, 0.23959606668601433, 0.2755213000345975)\n",
      "Total         min/avg/max: (3.5844426000257954, 7.2461331333421795, 11.897535700001754)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                 | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|-----------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  0 | models/gemini-2.5-pro | ok       |      3.309 |      7.002 |     11.651 |        0.207 |        0.240 |        0.276 |       3.584 |       7.246 |      11.898 |                 128 |         0.500 |                 1 |                   128 |\n",
      "\n",
      "--- models/gemini-2.5-pro ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-pro ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "Saved: gemini_sweep_thinking_budget.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirosh\\AppData\\Local\\Temp\\ipykernel_33424\\1043255840.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_budget = pd.concat(frames, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>ttfb_min</th>\n",
       "      <th>ttfb_avg</th>\n",
       "      <th>ttfb_max</th>\n",
       "      <th>t2last_min</th>\n",
       "      <th>t2last_avg</th>\n",
       "      <th>t2last_max</th>\n",
       "      <th>total_min</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_max</th>\n",
       "      <th>max_output_tokens</th>\n",
       "      <th>temperature</th>\n",
       "      <th>candidate_count</th>\n",
       "      <th>pro_thinking_budget</th>\n",
       "      <th>sweep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>models/gemini-2.5-pro</td>\n",
       "      <td>ok</td>\n",
       "      <td>3.308757</td>\n",
       "      <td>7.001742</td>\n",
       "      <td>11.651455</td>\n",
       "      <td>0.206696</td>\n",
       "      <td>0.239596</td>\n",
       "      <td>0.275521</td>\n",
       "      <td>3.584443</td>\n",
       "      <td>7.246133</td>\n",
       "      <td>11.897536</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>budget=128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-pro</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>budget=256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-pro</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>budget=512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model                                             status  \\\n",
       "0  models/gemini-2.5-pro                                                 ok   \n",
       "1  models/gemini-2.5-pro  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "2  models/gemini-2.5-pro  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "\n",
       "   ttfb_min  ttfb_avg   ttfb_max  t2last_min  t2last_avg  t2last_max  \\\n",
       "0  3.308757  7.001742  11.651455    0.206696    0.239596    0.275521   \n",
       "1       NaN       NaN        NaN         NaN         NaN         NaN   \n",
       "2       NaN       NaN        NaN         NaN         NaN         NaN   \n",
       "\n",
       "   total_min  total_avg  total_max  max_output_tokens  temperature  \\\n",
       "0   3.584443   7.246133  11.897536                128          0.5   \n",
       "1        NaN        NaN        NaN                128          0.5   \n",
       "2        NaN        NaN        NaN                128          0.5   \n",
       "\n",
       "   candidate_count  pro_thinking_budget       sweep  \n",
       "0                1                  128  budget=128  \n",
       "1                1                  256  budget=256  \n",
       "2                1                  512  budget=512  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_C = \"models/gemini-2.5-pro\"\n",
    "BUDGETS = [128, 256, 512]\n",
    "\n",
    "frames = []\n",
    "for b in BUDGETS:\n",
    "    df = bench_models([MODEL_C], PROMPT, max_output_tokens=128, temperature=0.5,\n",
    "                      candidate_count=1, pro_thinking_budget=b, runs=3)\n",
    "    df[\"sweep\"] = f\"budget={b}\"\n",
    "    frames.append(df)\n",
    "\n",
    "df_budget = pd.concat(frames, ignore_index=True)\n",
    "df_budget.to_csv(\"gemini_sweep_thinking_budget.csv\", index=False)\n",
    "print(\"\\nSaved: gemini_sweep_thinking_budget.csv\")\n",
    "df_budget.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93da2a",
   "metadata": {},
   "source": [
    "#### D) Candidate count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d674333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "TTFB          min/avg/max: (0.5611943999538198, 0.6692118332721293, 0.7514743999345228)\n",
      "Time-to-final min/avg/max: (0.4083305000094697, 0.42332870000973344, 0.43731800001114607)\n",
      "Total         min/avg/max: (0.9716687999898568, 1.0969886999810115, 1.1850864000152797)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                   | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|-------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  0 | models/gemini-2.5-flash | ok       |      0.561 |      0.669 |      0.751 |        0.408 |        0.423 |        0.437 |       0.972 |       1.097 |       1.185 |                 128 |         0.500 |                 1 |                     0 |\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Only one candidate can be specified in the current model', 'status': 'INVALID_ARGUMENT'}}\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Only one candidate can be specified in the current model', 'status': 'INVALID_ARGUMENT'}}\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "Saved: gemini_sweep_candidates.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirosh\\AppData\\Local\\Temp\\ipykernel_33424\\1470378233.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_cands = pd.concat(frames, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>ttfb_min</th>\n",
       "      <th>ttfb_avg</th>\n",
       "      <th>ttfb_max</th>\n",
       "      <th>t2last_min</th>\n",
       "      <th>t2last_avg</th>\n",
       "      <th>t2last_max</th>\n",
       "      <th>total_min</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_max</th>\n",
       "      <th>max_output_tokens</th>\n",
       "      <th>temperature</th>\n",
       "      <th>candidate_count</th>\n",
       "      <th>pro_thinking_budget</th>\n",
       "      <th>sweep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.561194</td>\n",
       "      <td>0.669212</td>\n",
       "      <td>0.751474</td>\n",
       "      <td>0.408331</td>\n",
       "      <td>0.423329</td>\n",
       "      <td>0.437318</td>\n",
       "      <td>0.971669</td>\n",
       "      <td>1.096989</td>\n",
       "      <td>1.185086</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>candidates=1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>400 INVALID_ARGUMENT. {'error': {'code': 400, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>candidates=2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>400 INVALID_ARGUMENT. {'error': {'code': 400, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>candidates=3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                                             status  \\\n",
       "0  models/gemini-2.5-flash                                                 ok   \n",
       "1  models/gemini-2.5-flash  400 INVALID_ARGUMENT. {'error': {'code': 400, ...   \n",
       "2  models/gemini-2.5-flash  400 INVALID_ARGUMENT. {'error': {'code': 400, ...   \n",
       "\n",
       "   ttfb_min  ttfb_avg  ttfb_max  t2last_min  t2last_avg  t2last_max  \\\n",
       "0  0.561194  0.669212  0.751474    0.408331    0.423329    0.437318   \n",
       "1       NaN       NaN       NaN         NaN         NaN         NaN   \n",
       "2       NaN       NaN       NaN         NaN         NaN         NaN   \n",
       "\n",
       "   total_min  total_avg  total_max  max_output_tokens  temperature  \\\n",
       "0   0.971669   1.096989   1.185086                128          0.5   \n",
       "1        NaN        NaN        NaN                128          0.5   \n",
       "2        NaN        NaN        NaN                128          0.5   \n",
       "\n",
       "   candidate_count  pro_thinking_budget         sweep  \n",
       "0                1                    0  candidates=1  \n",
       "1                2                    0  candidates=2  \n",
       "2                3                    0  candidates=3  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_D = \"models/gemini-2.5-flash\"\n",
    "CANDS = [1, 2, 3]\n",
    "\n",
    "frames = []\n",
    "for c in CANDS:\n",
    "    df = bench_models([MODEL_D], PROMPT, max_output_tokens=128, temperature=0.5,\n",
    "                      candidate_count=c, pro_thinking_budget=256, runs=3)\n",
    "    df[\"sweep\"] = f\"candidates={c}\"\n",
    "    frames.append(df)\n",
    "\n",
    "df_cands = pd.concat(frames, ignore_index=True)\n",
    "df_cands.to_csv(\"gemini_sweep_candidates.csv\", index=False)\n",
    "print(\"\\nSaved: gemini_sweep_candidates.csv\")\n",
    "df_cands.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954b3ed",
   "metadata": {},
   "source": [
    "#### E) Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8937e87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "TTFB          min/avg/max: (0.48513009992893785, 0.593769499954457, 0.6797960000112653)\n",
      "Time-to-final min/avg/max: (0.3512427000096068, 0.4996522666964059, 0.6030079000629485)\n",
      "Total         min/avg/max: (1.030003499938175, 1.0958349666325375, 1.2231318999547511)\n",
      "\n",
      "=== Successful models (sorted by total_avg) ===\n",
      "|    | model                   | status   |   ttfb_min |   ttfb_avg |   ttfb_max |   t2last_min |   t2last_avg |   t2last_max |   total_min |   total_avg |   total_max |   max_output_tokens |   temperature |   candidate_count |   pro_thinking_budget |\n",
      "|----|-------------------------|----------|------------|------------|------------|--------------|--------------|--------------|-------------|-------------|-------------|---------------------|---------------|-------------------|-----------------------|\n",
      "|  0 | models/gemini-2.5-flash | ok       |      0.485 |      0.594 |      0.680 |        0.351 |        0.500 |        0.603 |       1.030 |       1.096 |       1.223 |                 128 |         0.000 |                 1 |                     0 |\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "--- models/gemini-2.5-flash ---\n",
      "Error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.\n",
      "\n",
      "No successful rows to show.\n",
      "\n",
      "Saved: gemini_sweep_temperature.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirosh\\AppData\\Local\\Temp\\ipykernel_33424\\4124624620.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_temp = pd.concat(frames, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>ttfb_min</th>\n",
       "      <th>ttfb_avg</th>\n",
       "      <th>ttfb_max</th>\n",
       "      <th>t2last_min</th>\n",
       "      <th>t2last_avg</th>\n",
       "      <th>t2last_max</th>\n",
       "      <th>total_min</th>\n",
       "      <th>total_avg</th>\n",
       "      <th>total_max</th>\n",
       "      <th>max_output_tokens</th>\n",
       "      <th>temperature</th>\n",
       "      <th>candidate_count</th>\n",
       "      <th>pro_thinking_budget</th>\n",
       "      <th>sweep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>ok</td>\n",
       "      <td>0.48513</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>0.679796</td>\n",
       "      <td>0.351243</td>\n",
       "      <td>0.499652</td>\n",
       "      <td>0.603008</td>\n",
       "      <td>1.030003</td>\n",
       "      <td>1.095835</td>\n",
       "      <td>1.223132</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>temperature=0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>temperature=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>temperature=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>429 RESOURCE_EXHAUSTED. {'error': {'code': 429...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>temperature=0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                                             status  \\\n",
       "0  models/gemini-2.5-flash                                                 ok   \n",
       "1  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "2  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "3  models/gemini-2.5-flash  429 RESOURCE_EXHAUSTED. {'error': {'code': 429...   \n",
       "\n",
       "   ttfb_min  ttfb_avg  ttfb_max  t2last_min  t2last_avg  t2last_max  \\\n",
       "0   0.48513  0.593769  0.679796    0.351243    0.499652    0.603008   \n",
       "1       NaN       NaN       NaN         NaN         NaN         NaN   \n",
       "2       NaN       NaN       NaN         NaN         NaN         NaN   \n",
       "3       NaN       NaN       NaN         NaN         NaN         NaN   \n",
       "\n",
       "   total_min  total_avg  total_max  max_output_tokens  temperature  \\\n",
       "0   1.030003   1.095835   1.223132                128          0.0   \n",
       "1        NaN        NaN        NaN                128          0.2   \n",
       "2        NaN        NaN        NaN                128          0.5   \n",
       "3        NaN        NaN        NaN                128          0.9   \n",
       "\n",
       "   candidate_count  pro_thinking_budget            sweep  \n",
       "0                1                    0  temperature=0.0  \n",
       "1                1                    0  temperature=0.2  \n",
       "2                1                    0  temperature=0.5  \n",
       "3                1                    0  temperature=0.9  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_E = \"models/gemini-2.5-flash\"\n",
    "TEMPS = [0.0, 0.2, 0.5, 0.9]\n",
    "\n",
    "frames = []\n",
    "for t in TEMPS:\n",
    "    df = bench_models([MODEL_E], PROMPT, max_output_tokens=128, temperature=t,\n",
    "                      candidate_count=1, pro_thinking_budget=256, runs=3)\n",
    "    df[\"sweep\"] = f\"temperature={t}\"\n",
    "    frames.append(df)\n",
    "\n",
    "df_temp = pd.concat(frames, ignore_index=True)\n",
    "df_temp.to_csv(\"gemini_sweep_temperature.csv\", index=False)\n",
    "print(\"\\nSaved: gemini_sweep_temperature.csv\")\n",
    "df_temp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0288726",
   "metadata": {},
   "source": [
    "#### F) Streaming vs non-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cfb5711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- models/gemini-2.5-flash | streaming vs non-streaming ---\n",
      "Streaming:    TTFB avg = 2.4155947666149586 Time-to-final avg = 0.42524026668009657 Total avg = 2.8452505999788023\n",
      "Non-streaming:           Total avg = 1.0309919667197391\n"
     ]
    }
   ],
   "source": [
    "def run_once_nostream(model_id: str, prompt: str,\n",
    "                      max_output_tokens: int = None,\n",
    "                      temperature: float = None,\n",
    "                      candidate_count: int = 1,\n",
    "                      pro_thinking_budget: int = 256):\n",
    "    max_output_tokens = MAX_OUTPUT_TOKENS if max_output_tokens is None else max_output_tokens\n",
    "    temperature = TEMPERATURE if temperature is None else temperature\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    cfg = dict(\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        candidate_count=candidate_count,\n",
    "    )\n",
    "    if needs_thinking(model_id):\n",
    "        cfg[\"thinking_config\"] = types.ThinkingConfig(thinking_budget=pro_thinking_budget)\n",
    "    elif disallow_thinking(model_id):\n",
    "        pass\n",
    "    else:\n",
    "        cfg[\"thinking_config\"] = types.ThinkingConfig(thinking_budget=0)\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(**cfg),\n",
    "    )\n",
    "    total = time.perf_counter() - t0\n",
    "    return {\"ttfb_s\": math.nan, \"time_to_final_token_s\": math.nan, \"total_s\": total}\n",
    "\n",
    "def compare_stream_vs_nostream(model=\"models/gemini-2.5-flash\", runs=3):\n",
    "    print(f\"\\n--- {model} | streaming vs non-streaming ---\")\n",
    "    # warm-up\n",
    "    _ = run_once(model, PROMPT)\n",
    "    _ = run_once_nostream(model, PROMPT)\n",
    "\n",
    "    # measured\n",
    "    s = [run_once(model, PROMPT) for _ in range(runs)]\n",
    "    ns = [run_once_nostream(model, PROMPT) for _ in range(runs)]\n",
    "\n",
    "    def avg(xs, key):\n",
    "        vals = [v[key] for v in xs if not math.isnan(v[key])]\n",
    "        return sum(vals)/len(vals) if vals else math.nan\n",
    "\n",
    "    print(\"Streaming:    TTFB avg =\", avg(s, \"ttfb_s\"),\n",
    "          \"Time-to-final avg =\", avg(s, \"time_to_final_token_s\"),\n",
    "          \"Total avg =\", avg(s, \"total_s\"))\n",
    "    print(\"Non-streaming:           Total avg =\", avg(ns, \"total_s\"))\n",
    "\n",
    "compare_stream_vs_nostream()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cits5508",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
